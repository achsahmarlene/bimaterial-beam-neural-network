# -*- coding: utf-8 -*-
"""2019med1001_ME504_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZTXnlj9VEWTEafnk9n9ivO_VxfYF5tTX
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

import matplotlib.pyplot as plt
from tensorflow.keras import initializers

import random
from sklearn.model_selection import train_test_split

! pip install tensorflow-addons

from sklearn import preprocessing, svm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import tensorflow_addons as tfa
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

from google.colab import files
uploaded = files.upload()

X = pd.read_csv('input.csv')
Y = pd.read_csv('output.csv')
X = X.drop(X.columns[0], axis=1)
Y = Y.drop(Y.columns[0], axis=1)
print(Y.describe())

Y.iloc[:,1].plot(kind='kde')

X = np.array(X)
Y = np.array(Y)
print(X.shape, Y.shape)


np.random.seed(42)
random.seed(42)
# Dropping any rows with Nan values
x_ = X[:,4]
y_ = Y[:,4]
x_ = x_.reshape(-1,1)
y_ = y_.reshape(-1,1)

X_train, X_test, y_train, y_test = train_test_split(x_, y_, test_size = 0.1)

scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

np.random.seed(69)
# Splitting the data into training and testing data
regr = LinearRegression()

regr.fit(X_train, y_train)
y_pred = regr.predict(X_test)
print('Mean Square:', mean_squared_error(y_test, y_pred))
print('R2 Score:',regr.score(X_test, y_test))

from keras.models import Sequential , Model 
from keras.layers import Dense, Input, Activation
from tensorflow.keras.layers import LeakyReLU , ReLU

n1 = 10
n2 = 20

def run_regression(n_samples=1000):

    indices = random.sample(range(1000), n_samples)

    x_ = X[indices]
    y_ = Y[indices]

    training_input , testing_input, training_prediction , testing_prediction = train_test_split(x_, y_, test_size = .1)

    scaler = StandardScaler().fit(training_input)
    training_input = scaler.transform(training_input)
    testing_input = scaler.transform(testing_input)

    model =Sequential()

    model.add(Dense(n1, input_shape=(training_input.shape[1],)))  # first layer with input shape
    model.add(Dense(n2,))                                 

    model.add(Dense(5)) #output layer

    model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.01), loss='mse', metrics=tfa.metrics.r_square.RSquare())

    history = model.fit(training_input, training_prediction, epochs=50 , validation_data = (testing_input , testing_prediction), batch_size=8)

    return history

history = run_regression()
plt.plot(history.history['loss'],label= 'Training Loss')
plt.plot(history.history['val_loss'],label= 'Validation Loss')
plt.title('Training of u_y (lr=0.01)')
plt.xlabel('Epochs')
plt.ylabel('MSE Loss VALUE')
plt.legend()

"""Classification"""

Y_ = Y - np.mean(Y, axis=0)
Y_[Y_>0] = 1
Y_[Y_<0] = 0

def run_classification(n_samples=1000):

    indices = random.sample(range(1000),100)

    x_ = X[indices]
    y_ = Y_[indices]

    training_input , testing_input, training_prediction , testing_prediction = train_test_split(x_, y_, test_size = .1)

    scaler = StandardScaler().fit(training_input)
    training_input = scaler.transform(training_input)
    testing_input = scaler.transform(testing_input)

    model =Sequential()

    model.add(Dense(n1, input_shape=(training_input.shape[1],)))  # first layer with input shape
    model.add(Dense(n2,))                        

    model.add(Dense(5)) #output layer
    model.add(Activation('sigmoid'))

    model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['categorical_accuracy'])

    history = model.fit(training_input, training_prediction, epochs=50 , validation_data = (testing_input , testing_prediction), batch_size=8)

    return history

n_samples_list = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
losses_r = []
for n_samples in n_samples_list:
    print(n_samples)
    history_r = run_regression(n_samples)
    losses_r.append(history_r.history['loss'][-1])
plt.plot(n_samples_list, losses_r)
plt.show()

plt.plot(n_samples_list, losses_r, label='Training Loss')
plt.ylabel('loss value')
plt.xlabel('n_samples')
plt.title('Regression Analysis')
plt.legend()
plt.show()

losses_c = []
for n_samples in n_samples_list:
    print(n_samples)
    history_c = run_classification(n_samples)
    losses_c.append(history_c.history['loss'][-1])
plt.plot(n_samples_list, losses_c, label='Training Loss')
plt.ylabel('loss value')
plt.xlabel('n_samples')
plt.title('Classification Analysis')
plt.legend()
plt.show()

model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['categorical_accuracy'])

history = model.fit(training_input, training_prediction, epochs=10 , validation_data = (testing_input , testing_prediction), batch_size=8)

print(history.history['loss'][-1])

history = run_classification()
plt.plot(history.history['categorical_accuracy'],label= 'Training Categorical Accuracy')
plt.plot(history.history['val_categorical_accuracy'],label= 'Validation Categorical Accuracy')
plt.title('Training of Classifier (lr=0.01)')
plt.xlabel('Epochs')
plt.ylabel('Categorical Accuracy')
plt.legend()